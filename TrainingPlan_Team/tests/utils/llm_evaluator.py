import json
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI


def evaluate_with_llm(expected_output, generated_output, features=""):

    llm_json_mode = ChatOpenAI(temperature=0.0, model_name="gpt-4o-mini",
                           model_kwargs={"response_format": {"type": "json_object"}})
    background_story = """
    Your job is to compare two texts. The first is generated by an LLM, the second is the 
    expected output. You need to decide whether the generated text has the same meaning as the expected
    output. There might be additional information about expected features in the generated text. Return a json object 
    with the following keys:
    - correct: True if the generated text has the same meaning as the expected output, False otherwise.
    - explanation: A brief explanation of why the generated text has a different meaning than the expected output.
    """

    task_prompt = f"""
    Please evaluate the this generated text against the expected output.
    
    GENERATED TEXT
    -----
    {generated_output}
    
    EXPECTED OUTPUT
    -----
    {expected_output}
    
    FEATURES
    -----
    {features}
    """

    messages = [
        SystemMessage(content=background_story),
        HumanMessage(content=task_prompt.format(
            expected_output=expected_output,
            generated_output=generated_output,
            features=features
        ))
    ]

    evaluation = llm_json_mode.invoke(messages)
    eval_json = json.loads(evaluation.content)
    return eval_json